<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Marijn van Vliet" />
  <script src="random_lines.js" defer></script>
  <script src="code.js" defer></script>
  <script src="logoswitch.js" defer></script>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <canvas id="brainwaves"></canvas>
  <div id="code"></div>
  <main>
    <div id="logos">
      <a href="https://aalto.fi"><img src="images/logo_aalto1.svg" alt="Aalto University, School of Science" onload="logoSwitch(this)"/></a>
      <a href="https://www.aalto.fi/en/department-of-neuroscience-and-biomedical-engineering"><img src="images/logo_nbe.svg" alt="Department of Neuroimaging and Biomedical Engineering"/></a>
    </div>
    <h1>
      <span class="cap">N</span>euro-<span class="cap">C</span>omputational
      <span class="cap">I</span>nformation
      <span class="cap">P</span>rocessing
    </h1>

    <nav>
      <a href="#mission"><img src="images/nav_mission.svg">Mission</a>
      <a href="#members"><img src="images/nav_group.svg">Members</a>
      <a href="#projects"><img src="images/nav_projects.svg">Projects</a>
      <a href="#models"><img src="images/nav_model.svg">Models</a>
      <a href="#software"><img src="images/nav_software.svg">Software</a>
      <a href="#talks"><img src="images/nav_talks.svg">Talks</a>
      <!--
      <a href="participate.html"><img src="images/nav_participate.svg">Participate</a>
      -->
    </nav>

    <section>
    <h2 id="mission"><span class="cap">M</span>ission</h2>
    <p>It is our mission to understand how the brain processes information: how information enters it, how low-level information gets processed into higher-level information, how it integrates with other information, and how decisions are made based on it.
    </p>
    <p>
      Aalto University is famous for its pioneering work in brain imaging methods development.
      It is one of the birthplaces of MEG and is now performing ground breaking work in optically pumped magnetometer (OPM) sensor technology to drive the possibilities even further.
      This group combines experimental work using these imaging methods with advanced data analysis and formal computational modeling to piece together the various information streams throughout the brain.
    </p>
    </section>

    <section>
      <h2 id="members"><span class="cap">L</span>ab <span class="cap">M</span>embers</h2>
      <div class="member">
        <h3>Shristi Baral</h3>
        <h4>PhD Candidate</h4>
        <div class="links">
          <a href="https://github.com/shristibaral">github</a>
          <a href="#">publications</a>
          <a href="mailto:shristi.baral@aalto.fi">email</a>
        </div>
        <div class="image-cropper">
            <img src="images/shristi.jpg">
        </div>
        <p>
          Creating semantic representations through integration of multiple modality-specific processing streams.
        </p>
      </div>

      <div class="member">
        <h3>Jiaxin You</h3>
        <h4>PhD Candidate</h4>
        <div class="links">
          <a href="#">github</a>
          <a href="#">publications</a>
          <a href="mailto:jiaxin.you@aalto.fi">email</a>
        </div>
        <div class="image-cropper">
            <img src="images/jiaxin.jpg">
        </div>
        <p>
          Resolution of misspelled words through top-down connections.
          Also part of the <a href="https://www.aalto.fi/en/department-of-neuroscience-and-biomedical-engineering/imaging-language">Imaging Language</a> group.
        </p>
      </div>

      <div class="member">
        <h3>Marijn van Vliet</h3>
        <h4>Academy Research Fellow</h4>
        <div class="links">
          <a href="https://github.com/wmvanvliet">github</a>
          <a href="https://scholar.google.fi/citations?user=fqNH86EAAAAJ&hl=en&oi=ao">publications</a>
          <a href="mailto:marijn.vanvliet@aalto.fi">email</a>
        </div>
        <div class="image-cropper">
            <img src="images/marijn.jpg">
        </div>
        <p>
          Linking neuro-computational processing streams to neuroimaging data.
        </p>
      </div>
    </section>

    <section>
      <h2 id="projects"><span class="cap">R</span>esearch <span class="cap kern">P</span>rojects</h2>
      <div class="project">
        <h3><a href="noisy-language/index.html">Misspelled-word reading modulates late cortical dynamics</a></h3>
        <a class="poster" href="noisy-language/poster.pdf"><img src="noisy-language/poster_thumb.png"></a>
        <h4>Jiaxin You, Aino Saranpää, Tiina Lindh-Knuutila, Marijn van Vliet, Riitta Salmelin</h4>
        <p>
          Literate humans can effortlessly interpret tens of thousands of words, even when the words are sometimes written incorrectly. This phenomenon suggests a flexible nature of reading that can endure a certain amount of noise. In this study, we investigated where and when brain responses diverged for conditions where misspelled words were resolved as real words or not. We used magnetoencephalography (MEG) to track the cortical activity as the participants read words with different degrees of misspelling that were perceived to range from real words to complete pseudowords, as confirmed by their behavioral responses. In particular, we were interested in how lexical information survives (or not) along the uncertainty spectrum, and how the corresponding brain activation patterns evolve spatiotemporally. We identified three brain regions that were notably modulated by misspellings: left ventral occipitotemporal cortex (vOT), superior temporal cortex (ST), and precentral cortex (pC). This suggests that resolving misspelled words into stored concepts involves an interplay between orthographic, semantic, and phonological processing. Temporally, these regions showed fairly late and sustained responses selectively to misspelled words. Specifically, an increasing level of misspelling increased the response in ST from 300 ms after stimulus onset; a functionally fairly similar but weaker effect was observed in pC. In vOT, misspelled words were sharply distinguished from real words notably later, after 700 ms. A linear mixed effects (LME) analysis further showed that pronounced and long-lasting misspelling effects appeared first in ST and then in pC, with shorter-lasting activation also observed in vOT. We conclude that reading misspelled words engages brain areas typically associated with language processing, but in a manner that cannot be interpreted merely as a rapid feedforward mechanism. Instead, feedback interactions likely contribute to the late effects observed during misspelled-word reading.
        </p>
        <p><a href="noisy-language/index.html">Find out more on the project page...</a></p>
      </div>
      <div class="project">
        <h3><a href="viswordrec-baseline/index.html">Convolutional networks can model the functional modulation of MEG responses during reading</a></h3>
        <a class="poster" href="viswordrec-baseline/poster.pdf"><img src="viswordrec-baseline/poster_thumb.png"></a>
        <h4>Marijn van Vliet, Oona Rinkinen, Takao Shimizu, Anni-Mari Niskanen, Barry Devereux, Riitta Salmelin</h4>
        <p>
          To better understand the computational steps that the brain performs during reading, we used a convolutional neural network as a computational model of visual word recognition, the first stage of reading.
          In contrast to traditional models of reading, our model directly operates on the pixel values of an image containing text, and has a large vocabulary of 10k Finnish words.
          The same stimuli can thus be presented unmodified to both the model and human volunteers in an MEG scanner.
          In a direct comparison between model and brain activity, we show that the model accurately predicts the amplitude of three evoked MEG response components commonly observed during reading.
          We conclude that the deep learning techniques that revolutionized models of object recognition can also create models of reading that can be straightforwardly compared to neuroimaging data, which will greatly facilitate testing and refining theories on language processing in the brain.
        </p>
        <p><a href="viswordrec-baseline/index.html">Find out more on the project page...</a></p>
      </div>
      <div class="project">
        <h3><a href="fusion/index.html">The visual processing streams for objects and words may interact in the human brain</a></h3>
        <a class="poster" href="fusion/poster.pdf"><img src="fusion/poster_thumb.png"></a>
        <h4>Shristi Baral, Riitta Salmelin, Marijn van Vliet</h4>
        <p>
          The brain's ability to process written words and objects has been well-studied, but often in isolation. A key question remains: how does orthographic processing interact with visual object recognition, given that both share the same pathways? Since object recognition is already developed when we learn to read as a child, the visual system must reorganize to accommodate written words, enabling both modalities to coexist and integrate efficiently. Written words derive meaning from real-world objects and concepts, requiring the brain’s ventral stream to reorganise and process both modalities.
          The question is how written words (e.g., "CUP") and visual objects (e.g., a picture of a cup) lead to the same amodal neural representation. We investigated this using Representational Similarity Analysis (RSA) between a CNN-based fusion model and MEG data.
        </p>
        <p><a href="fusion/index.html">Find out more on the project page...</a></p>
      </div>
    </section>

    <section>
    <h2 id="models"><span class="cap">M</span>odels</h2>
    <p>The CMHC lab maintains a set of models that you can interact with through your browser.
      <div class="model">
        <h3><a href="word2vec/index.html">Word2Vec Guessing Game</h3></a>
        <p>
          Think of a target word and give "clue" words below that describe the target. The word2vec model will give the 10 words closest to the semantic mean of the given clues. See if you can make the computer guess your chosen target word. 
        </p>
        <div class="links">
          <a href="word2vec/index.html">English version</a>
          <a href="word2vec/nearest-fi.html">Finnish version</a>
        </div>
      </div>
      <div class="model">
        <h3><a href="word2vec/comparison.html">Word2Vec Semantic Projection</h3></a>
        <p>
          Describe a dimension by thinking of words that create a contrast. Make sure to enter them "pairwise", meaning that the first negative word is the antonym of the first positive word. Finally, enter some words to be compared along the dimension.
        </p>
        <div class="links">
          <a href="word2vec/comparison.html">English version</a>
          <a href="word2vec/comparison-fi.html">Finnish version</a>
        </div>
      </div>
    </section>

    <section>
      <h2 id="software"><span class="cap">S</span>oftware</h2>
      <div class="package">
        <h3><a href="http://users.aalto.fi/~vanvlm1/mne-rsa">MNE-RSA</a></h3>
        <h4>Representational similarity analysis for MNE-Python</h4>
        <div class="links">
          <a href="https://mne.tools/mne-rsa">website</a>
          <a href="https://github.com/mne-tools/mne-rsa">github</a>
        </div>
        <p>
          A plugin for <a href="https://mne.tools">MNE-Python</a> to perform representational similarity analysis (RSA) on EEG &amp; MEG data in a searchlight fashion.
          It includes best practice features such as cross-validation and PCA preprocessing. 
        </p>
      </div>

      <div class="package">
        <h3><a href="https://github.com/wmvanvliet/pytorch_hmax">Pytorch-HMAX</a></h3>
        <h4>The HMAX model of vision implemented in PyTorch</h4>
        <div class="links">
          <a href="https://github.com/wmvanvliet/pytorch_hmax">github</a>
        </div>
        <p>
          A PyTorch implementation of the HMAX model that closely follows that of the MATLAB implementation of The Laboratory for Computational Cognitive Neuroscience of Georgetown University.
        </p>
      </div>

      <div class="package">
        <h3><a href="https://github.com/wmvanvliet/posthoc">Post-Hoc</a></h3>
        <h4>Inject domain information into your scikit-learn models</h4>
        <div class="links">
          <a href="https://aaltoimaginglanguage.github.io/posthoc/index.html">website</a>
          <a href="https://github.com/wmvanvliet/posthoc">github</a>
        </div>
        <p>
          Post-hoc modification of linear models is a way to decompose a linear scikit-learn model into three matrices: the pattern, the covariance, and the normalizer. These matrices can be modified at will to inject domain information and then re-assembled into a linear model.
        </p>
      </div>

      <div class="package">
        <h3><a href="https://github.com/wmvanvliet/torch-pcdim">Torch-PCDIM</a></h3>
        <h4>The DIM predictive coding model implemented in PyTorch</h4>
        <div class="links">
          <a href="https://github.com/wmvanvliet/torch-pcdim">github</a>
        </div>
        <p>
        A PyTorch implementation of DIM-stype predictive coding where each layer is predicting the preceding layer while propagating prediction error forwards. It can be used to simulate evoked responses. Includes an implementation of <a href="https://doi.org/10.1016/j.cognition.2024.105755">Nour Eddine et al. (2024)</a>.
        </p>
      </div>

    <section>
      <h2 id="talks"><span class="cap kern">T</span>alks &amp; <span class="cap">L</span>ectures</h2>
      <a class="talk" href="https://youtu.be/19cNdk6pjrw"><img src="images/talk6.jpg"></a>
      <a class="talk" href="https://youtu.be/kLeRrNT26qc"><img src="images/talk5.jpg"></a>
      <a class="talk" href="https://youtu.be/Cqbjz6eHTLc"><img src="images/talk1.jpg"></a>
      <a class="talk" href="https://youtu.be/2y68NMmacG4"><img src="images/talk2.jpg"></a>
      <a class="talk" href="https://youtu.be/RWpCWPZqdj8"><img src="images/talk3.jpg"></a>
      <a class="talk" href="https://youtu.be/OlxVhkuiGPU"><img src="images/talk4.jpg"></a>
    </section>
  </main>
</body>
</html
