<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Marijn van Vliet" />
  <meta name="author" content="Oona Rinkinen" />
  <meta name="author" content="Takao Shimizu" />
  <meta name="author" content="Barry Devereux" />
  <meta name="author" content="Riitta Salmelin" />
  <title>A large-scale computational model to accurately predict early brain activity in response to written words</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <link rel="stylesheet" href="vanvliet.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script type="text/javascript" src="vanvliet.js"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">A large-scale computational model to accurately predict early brain activity in response to written words</h1>
<p class="author">Marijn van Vliet</p>
<p class="author">Oona Rinkinen</p>
<p class="author">Takao Shimizu</p>
<p class="author">Barry Devereux</p>
<p class="author">Riitta Salmelin</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#results">Results</a>
<ul>
<li><a href="#the-brain">The brain</a></li>
<li><a href="#the-model">The model</a></li>
<li><a href="#comparing-model-and-brain">Comparing model and brain</a></li>
</ul></li>
<li><a href="#discussion">Discussion</a>
<ul>
<li><a href="#limitations-of-the-current-model">Limitations of the current model</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul></li>
<li><a href="#methods">Methods</a>
<ul>
<li><a href="#meg-study-design-and-data-analysis">MEG study design and data analysis</a></li>
<li><a href="#model">Model</a></li>
<li><a href="#comparison-between-model-and-meg-data">Comparison between model and MEG data</a></li>
<li><a href="#data-availability">Data availability</a></li>
</ul></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#references">References</a></li>
</ul>
</nav>
<h1 id="introduction">Introduction</h1>
<p><span>What</span> computational operations is the brain performing when it recognizes some splotches of ink on a piece of paper as a meaningful word? This question has been the focus of a large number of neuroimaging studies that examine brain activity during reading. Noninvasive measurement techniques such as <span>electroencephalography (EEG)</span><span class="citation" data-cites="Grainger2009"> (Grainger and Holcomb 2009)</span>, <span>MEG</span><span class="citation" data-cites="Salmelin2007"> (Salmelin 2007)</span> and <span>functional magnetic resonance imaging (fMRI)</span><span class="citation" data-cites="Price2012"> (Price 2012)</span> have provided a wealth of information about when and where changes in activity might be expected during various tasks involving orthographic processing<span class="citation" data-cites="Carreiras2014"> (Carreiras et al. 2014)</span>. However, these observations alone are unlikely to lead to a complete understanding of the computations performed by the brain during reading<span class="citation" data-cites="Poeppel2012"> (Poeppel 2012)</span>. To develop such an understanding, we need to make these computations explicit, model them, and test and refine the model against the neural signatures observed during imaging studies<span class="citation" data-cites="Barber2007 Price2018"> (Barber and Kutas 2007; Price 2018)</span>. In this study, we attempted to model the macro-level computations — that is, the net result of thousands of individual biological neurons working together — that manifest as evoked components in <span>EEG</span>/<span>MEG</span> data. We demonstrate a large-scale model of visual word recognition that enabled us, for the first time, to present to the model the same set of bitmap images that were presented to human volunteers in an earlier study<span class="citation" data-cites="Vartiainen2011"> (Vartiainen et al. 2011)</span>, and thereby directly compare the activation inside the model with <span>MEG</span> data.</p>
<p>In past decades, several computational models have been created that successfully capture some aspects of visual word recognition. Amongst the first such models, the <span>interactive activation and competition (IAC)</span> model of letter perception, by McClelland and Rumelhart<span class="citation" data-cites="McClelland1981 Rumelhart1982"> (McClelland and Rumelhart 1981; Rumelhart and McClelland 1982)</span>, showed how the interplay of bottom-up and top-down connections results in a system capable of "filling in the blanks" when faced with a partially obscured word. This model was later extended to model semantics as well, showing how the activation of some semantic features ("is alive", "is a bird") leads to the subsequent activation of more semantic features ("can fly", "lays eggs"), in what became known as the <span>parallel distributed processing (PDP)</span> framework<span class="citation" data-cites="McClelland2003"> (McClelland and Rogers 2003)</span>. Note that while these models consist of many interconnected units, those units and their connections do not aim to model a biological connectome, but are an abstract representation of macro-level computations performed by one. <span class="citation" data-cites="Coltheart2001">Coltheart et al. (2001)</span> pointed out the benefits of explicitly defined connections in their <span>dual-route cascaded (DRC)</span> model of visual word recognition and reading out loud, as they grant the researcher exact control over the macro-level computations. However, as the scope of the models increases, "hand-wiring" the connections in the model becomes increasingly difficult. Therefore, most current models employ back-propagation to learn the connection weights between units based on a large training dataset. Together, <span>PDP</span> and <span>DRC</span> models have been shown to account for many behavioral findings, such as reaction times and recognition errors, in both healthy volunteers and patients<span class="citation" data-cites="McLeod2000 McClelland2003 Perry2007"> (McLeod, Shallice, and Plaut 2000; McClelland and Rogers 2003; Perry, Ziegler, and Zorzi 2007)</span>. However, in addition to verifying the eventual output of the model against observed behavior, it would seem informative and important to also verify the intermediate computations by comparing the internal state of the model against recordings of brain activity. Unfortunately, none of these models can produce detailed quantitative data that can be directly compared with neuroimaging data, leaving researchers to focus on finding indirect evidence that the same macro-level computations occur in both the model and the brain, with varying levels of success<span class="citation" data-cites="Jobard2003 Protopapas2016 Barber2007"> (Jobard, Crivello, and Tzourio-Mazoyer 2003; Protopapas et al. 2016; Barber and Kutas 2007)</span>.</p>
<p>More recently, progress has been made towards models whose internal state can be more easily compared with data obtained in neuroimaging studies. <span class="citation" data-cites="Laszlo2012">Laszlo and Plaut (2012)</span> took an important step by creating a <span>PDP</span>-style model of visual word recognition that performs the computations required to translate a collection of letters (<span class="sans-serif">e.g., DOG</span>) into an abstract semantic representation of a concept (e.g., a furry animal that barks), while producing an <span>EEG</span>-like signal<span class="citation" data-cites="Laszlo2014"> (Laszlo and Armstrong 2014)</span>. By summing the activity of the computational units in specific layers of the model, the resulting time-varying signal resembles a well-known signal component, observed in <span>EEG</span> and <span>MEG</span> studies, referred to as the N400 potential<span class="citation" data-cites="Kutas2011"> (Kutas and Federmeier 2011)</span>. However, while this model produces a neuroimaging-like signal, it still cannot be numerically compared with real neuroimaging data, since the simulated environment in the model is extremely simplified (e.g. it operates exclusively on 3-letter words), whereas the brain data will, by nature, reflect the reading process in more varied and realistic visual settings.</p>
<p>In addition to using a small and restricted vocabulary, another striking simplification made in both <span>PDP</span>-based and <span>DRC</span> models is in how the initial visual input is presented to the model. The models make use of "letter banks", where each letter of a written word is encoded in as separate group of inputs. The letters themselves are encoded as either a collection of 16 predefined line segments<span class="citation" data-cites="McClelland1981"> (McClelland and Rumelhart 1981)</span> or a binary code indicating the letter<span class="citation" data-cites="Laszlo2012"> (Laszlo and Plaut 2012)</span>. This rather high level of visual representation sidesteps having to deal with issues such as visual noise, letters with different scales, rotations and fonts, segmentation of the individual letters, and so on. However, in studies of brain activity, these processes are thought to contribute a major part to the activity observed during the first 200 ms in <span>EEG</span>/<span>MEG</span> studies of visual word recognition<span class="citation" data-cites="Salmelin2007 Grainger2009 Tarkiainen1999 Petit2006 Pammer2004"> (Salmelin 2007; Grainger and Holcomb 2009; Tarkiainen et al. 1999; Petit et al. 2006; Pammer et al. 2004)</span>, and to the activity observed in the primary visual cortex and <span>visual word form area (VWFA)</span> through <span>fMRI</span><span class="citation" data-cites="Vinckier2007 Cohen2004"> (Vinckier et al. 2007; Cohen and Dehaene 2004)</span>.</p>
<p>Recent advances in deep learning and its software ecosystem are rapidly changing our notion of what is computationally tractable to model<span class="citation" data-cites="Richards2019 LeCun2015"> (Richards et al. 2019; LeCun, Bengio, and Hinton 2015)</span>. <span>convolutional neural networks (CNNs)</span> have emerged that perform scale- and rotation-invariant visual object recognition at very high levels of accuracy<span class="citation" data-cites="Krizhevsky2017 Simonyan2015 Dai2021"> (Krizhevsky, Sutskever, and Hinton 2017; Simonyan and Zisserman 2015; Dai et al. 2021)</span>. Furthermore, they model some functions of the visual cortex well enough that a direct comparison between network state and neuroimaging data has become possible<span class="citation" data-cites="Schrimpf2018 Devereux2018 Yamins2016"> (Schrimpf et al. 2018; Devereux, Clarke, and Tyler 2018; Yamins and DiCarlo 2016)</span>. Consequently, our understanding of basic visual processing in the brain has increased tremendously<span class="citation" data-cites="Lindsay2020"> (Lindsay 2020)</span>. Since the first stages of reading, namely visual word recognition, can be seen as a specialized form of object recognition, <span>CNNs</span> may very well be a suitable tool for increasing the scale of traditional connectionist models of reading, allowing for a much more realistic modeling of the early orthographic processing steps than what has been possible so far. Indeed, earlier work has established that a <span>CNN</span> can form internal letter representations<span class="citation" data-cites="Testolin2017"> (Testolin, Stoianov, and Zorzi 2017)</span> and can be coerced into forming a <span>VWFA</span>-like region on top of an existing visual system<span class="citation" data-cites="Hannagan2021"> (Hannagan et al. 2021)</span>. In this study, we investigated whether a <span>CNN</span> can produce a signal that can be directly compared to neuroimaging data, in this case <span>MEG</span>, during a single-word reading task.</p>
<p>We trained a <span class="smallcaps">vgg</span>-11 network<span class="citation" data-cites="Simonyan2015"> (Simonyan and Zisserman 2015)</span> to recognize 10 000 different Finnish words. Next, we ran through the model a set of stimuli that had been used in an earlier <span>MEG</span> study on human volunteers<span class="citation" data-cites="Vartiainen2011"> (Vartiainen et al. 2011)</span>. These stimuli consisted of valid Finnish words, pseudowords and consonant strings, and two stimulus types that traditional <span>PDP</span> and <span>DRC</span> models cannot handle, namely letter-like symbol strings, and words embedded in visual noise. We demonstrate that various layers in the model behave similarly to several well-studied evoked responses and evaluate this similarity both qualitatively and numerically.</p>
<h1 id="results">Results</h1>
<h2 id="the-brain">The brain</h2>
<p>For this study, we reused <span>MEG</span> data that was collected as part of a study by <span class="citation" data-cites="Vartiainen2011">Vartiainen et al. (2011)</span>. During the recording session, 15 participants were presented with 560 orthographic stimuli in a silent reading task (Figure 1A). The stimuli were designed to form a series of experimental contrasts that highlight three processing stages during single word reading: early visual processing, letter shape detection and lexical access.</p>
<p>To summarize the high-dimensional <span>MEG</span> data, the sensor-level signals were segregated into cortical-level spatiotemporal components through guided <span>equivalent current dipole (ECD)</span> modeling<span class="citation" data-cites="Hansen2010"> (Hansen, Kringelbach, and Salmelin 2010)</span>. The resulting <span>ECDs</span> were grouped based on their location and the timing of peak signal strength. The current study re-uses three of such <span>ECD</span> groups defined in the original study<span class="citation" data-cites="Vartiainen2011"> (Vartiainen et al. 2011)</span> along the ventral stream (Figure 1B). Together, the grand-average time courses of signal strength at these <span>ECD</span> groups (Figure 1C) capture the different processing stages that the original study sought to highlight.</p>
<figure>
<img src="dipole_timecourses.png" id="fig:brain" alt=" Figure 1: Summary of the meg results obtained by Vartiainen et al. (2011). A: Examples of stimuli used in the MEG experiment. Each stimulus contained 7–8 letters or symbols. B: Source estimate of the evoked MEG activity, using MNE-dSPM. The grand-average activity to word stimuli, averaged for three time intervals, is shown in orange hues. For each time interval, white circles indicate the location of the most representative left-hemisphere ECD for each participant, as determined by Vartiainen et al. (2011). C: Grand-average time course of signal strength for each group of ECDs in response to the different stimulus types. The traces are color-coded to indicate the stimulus type as shown in A. " /><figcaption aria-hidden="true"> <strong>Figure 1: Summary of the <span class="smallcaps"><strong>meg</strong></span> results obtained by <span class="citation" data-cites="Vartiainen2011">Vartiainen et al. (2011)</span></strong>.<br />
<strong>A:</strong> Examples of stimuli used in the <span>MEG</span> experiment. Each stimulus contained 7–8 letters or symbols.<br />
<strong>B:</strong> Source estimate of the evoked <span>MEG</span> activity, using <span data-acronym-label="MNE-dSPM" data-acronym-form="singular+short">MNE-dSPM</span>. The grand-average activity to word stimuli, averaged for three time intervals, is shown in orange hues. For each time interval, white circles indicate the location of the most representative left-hemisphere <span>ECD</span> for each participant, as determined by <span class="citation" data-cites="Vartiainen2011">Vartiainen et al. (2011)</span>.<br />
<strong>C:</strong> Grand-average time course of signal strength for each group of <span>ECDs</span> in response to the different stimulus types. The traces are color-coded to indicate the stimulus type as shown in <strong>A</strong>. </figcaption>
</figure>
<p>The first group of <span>ECDs</span> is occipitally located and represents early onset activity in the visual cortex, peaking at 65–115 ms after the stimulus onset. The signal strength at these <span>ECDs</span> is driven by the visual complexity of the stimulus and is characterized in this study by a large response to noise-embedded words relative to all other stimulus types. Following <span class="citation" data-cites="Tarkiainen1999">Tarkiainen et al. (1999)</span>, we will refer to this effect as the Type I response.</p>
<p>The second group is located further along the fusiform gyrus, with the signal strength peaking slightly later at 140–200 ms. <span>ECDs</span> belonging to this group exhibit sensitivity to whether the stimulus contains letters that are part of the participant’s native alphabet<span class="citation" data-cites="Tarkiainen1999"> (Tarkiainen et al. 1999)</span>, and is characterized in this study by a smaller response to stimuli containing symbol strings relative to stimuli that contain letters. Note that, in contrast to the Type I response, adding noise to the stimulus decreases the signal for these <span>ECDs</span>, further indicating that the underlying cognitive computations are more selective towards specific visual features. Following <span class="citation" data-cites="Tarkiainen1999">Tarkiainen et al. (1999)</span>, we will refer to this as the Type II response. The Type II response is located in the same general cortical region as the <span>VWFA</span> reported using <span>fMRI</span><span class="citation" data-cites="Cohen2004"> (Cohen and Dehaene 2004)</span>, but they are not spatially or functionally identical<span class="citation" data-cites="Vartiainen2011"> (Vartiainen et al. 2011)</span>.</p>
<p>The third and final <span>ECD</span> group is located in the temporal cortex, peaking much later at 300–500 ms. Its signal strength is modulated by the lexical content of the stimulus, manifested in this study as a larger response to the word-like (i.e., words and pseudowords) versus the non-word-like stimuli. The location and response pattern of this <span>ECD</span> group corresponds to that of the N400m<span class="citation" data-cites="Halgren2002 Helenius1998b Service2007 Salmelin2007"> (Halgren et al. 2002; Helenius et al. 1998; Service et al. 2007; Salmelin 2007)</span>.</p>
<h2 id="the-model">The model</h2>
<p>As a model of the computations underlying the brain activity observed during the <span>MEG</span> experiment, we used a <span class="smallcaps">vgg</span>-11<span class="citation" data-cites="Szegedy2015"> (Szegedy et al. 2015)</span> network architecture, pretrained on ImageNet<span class="citation" data-cites="Russakovsky2015"> (Russakovsky et al. 2015)</span>. This architecture consists of five convolution layers (three of which perform convolution twice), followed by two densely connected layers, terminating in an output layer (Figure 2A). The model was trained to perform visual word recognition using a training set that contained 1 000 000 images containing a word, rendered in varying fonts, sizes and rotations, with varying degrees of visual background noise (Figure 2B). The vocabulary size (10 000) was chosen to exceed the number of units in the densely connected layers (4 096), forcing the model to construct a sub-lexical representation. To add a detection element to what is otherwise a discrimination task, an additional 50 000 images containing only noise were added to the training set. The task for the model was to identify the correct word (regardless of the font, size and rotation used to render the text) by setting the corresponding unit in the output layer to a high value. If no word was present in the image, all output units should have a low value. This was achived be replacing the output layer of the pre-trained model with one containing 10 001 randomly initialized units for word recognition training, were the final unit was trained to denote the "no word present" condition, and removing this final unit after training was complete. During training, the performance of the model was evaluated on an independent test set of 100 000 images that contained words and 5 000 that contained only noise. Training was stopped when the model’s performance plateaued, at which point the accuracy on the test set was 99.3 %.</p>
<figure>
<img src="model.png" id="fig:model" alt=" Figure 2: Overview of the proposed computational model of visual word recognition in the brain. A: The vgg-11 model architecture, consisting of five convolution layers, two fully connected layers and one output layer. B: Examples of the images used to train the model. " /><figcaption aria-hidden="true"> <strong>Figure 2: Overview of the proposed computational model of visual word recognition in the brain.</strong><br />
<strong>A:</strong> The <span class="smallcaps">vgg</span>-11 model architecture, consisting of five convolution layers, two fully connected layers and one output layer.<br />
<strong>B:</strong> Examples of the images used to train the model. </figcaption>
</figure>
<p>After training, the model was used to classify the stimulus images presented during the <span>MEG</span> experiment (see Supplementary Table 1). The model classified 114 of the 118 word stimuli correctly (accuracy 96.6 %). The 5 incorrectly classified stimuli were classified as close orthographic neighbors (e.g. <span class="sans-serif">LUOMINEN</span><span class="math inline">→</span><span class="sans-serif">TUOMINEN</span>). All noise-embedded words were misclassified, since the amount of noise made them unrecognizable, with 99 out of 118 being classified as the word <span class="sans-serif">METSÄTEOLLISUUS</span>, which is one of the words in the training set with the highest visual complexity. Since the model was only trained to detect and discriminate between words, all non-word stimuli (pseudowords, consonant strings, symbol strings) were misclassified as well. In this case, it was more difficult to find a pattern in the way they were classified. In many cases, similarity in letter shape seemed to play a role (e.g. <span class="sans-serif">SKKNTMT</span><span class="math inline">→</span><span class="sans-serif">SUKUNIMI</span>, <span class="sans-serif">ÄHKÄÄJÄ</span><span class="math inline">→</span><span class="sans-serif">VARAAJA</span>), but not always (e.g. <span class="sans-serif">INKRIHTI</span><span class="math inline">→</span><span class="sans-serif">EMOLEVY</span>).</p>
<figure>
<img src="activations.png" id="fig:contribution" alt=" Figure 3: Contribution maps For each type of stimulus (words, pseudowords, consonants, symbols and noise embedded words), Deeplift contribution maps are shown for five exemplars. For each exemplar, the original word in the stimulus is given, followed by the word corresponding to the node in the output layer that was activated the strongest (i.e., the model prediction). The maps indicate the amount of influence each pixel has on this output node (red=positive, blue=negative, in arbitrary units). " /><figcaption aria-hidden="true"> <strong>Figure 3: Contribution maps</strong><br />
For each type of stimulus (words, pseudowords, consonants, symbols and noise embedded words), Deep<span class="smallcaps">lift</span> contribution maps are shown for five exemplars. For each exemplar, the original word in the stimulus is given, followed by the word corresponding to the node in the output layer that was activated the strongest (i.e., the model prediction). The maps indicate the amount of influence each pixel has on this output node (red=positive, blue=negative, in arbitrary units). </figcaption>
</figure>
<p>To better understand what kind of visual features the model used to determine the output word, we performed Deep<span class="smallcaps">lift</span><span class="citation" data-cites="Shrikumar2019"> (Shrikumar, Greenside, and Kundaje 2019)</span> analysis to determine the contribution of each input pixel to the strongest activated output node (Figure 3). We observed similar contribution patterns for each individual letter, no matter its location in the string, indicating that the model learned to use individual letter shapes as building blocks.</p>
<h2 id="comparing-model-and-brain">Comparing model and brain</h2>
<p>To directly compare layer activations in the model with the brain responses in word reading (Figure 4), we collected summary statistics, here referred to as "response patterns", for the <span>ECD</span> time courses, <span data-acronym-label="MNE-dSPM" data-acronym-form="singular+short">MNE-dSPM</span> source estimates, and the <span>rectified linear unit (ReLu)</span> activations in the model. For each <span>ECD</span>, the response pattern was obtained by integrating the signal strength in response to each stimulus over the time window used in <span class="citation" data-cites="Vartiainen2011">Vartiainen et al. (2011)</span> for statistical analysis, followed by a z-transformation (Figure 4A). The single-participant response patterns were then averaged to obtain the grand-average response patterns per stimulus. For the model, response patterns were obtained for each layer, by computing the mean <span>ReLu</span> activation across all the units in the layer in response to each stimulus, followed by a z-transformation (Figure 4B). Finally, response patterns were obtained for each source/time point in the grand-average <span data-acronym-label="MNE-dSPM" data-acronym-form="singular+short">MNE-dSPM</span> source estimates by computing the magnitude of the estimated signal at that point (Figure 4C).</p>
<p>Since the same individual stimuli were processed by both the model and the brain, we can measure the model-brain correspondence qualitatively by examining the overall shape of the response patterns, and quantitatively by computing the correlation between them (Figure 4, Table 1). Note that, in addition to real words with and without noise and pseudowords, the stimuli also included consonant and symbol strings, which the model was not trained to recognize.</p>
<figure>
<img src="results_small.png" id="fig:results" alt=" Figure 4: Grand-average comparison between brain and model. Based on the response patterns to each stimulus type, three processing stages were identified that correspond to different time windows in the MEG activity and different layer types in the model. A: Grand-average response patterns obtained from evoked MEG activity, quantified using three ECD groups. For each stimulus, the ECD time courses were integrated over the indicated time interval. B: Response patterns obtained from the model. For each layer, the mean ReLu activation in response to each stimulus. Matching response patterns between ECDs and model layers are shown using black lines, accompanied by the corresponding Pearson correlations. Two additional correlation values are shown in gray, for which the response patterns do not match, but which are discussed in the main text. C: Pearson correlation between the model and the MNE-dSPM source estimate. Grand-average source estimates were obtained in response to each stimulus. The correlation map was obtained by correlating the activity at each source point with that for each layer of the model. The correlation map is shown at the time of peak correlation (using the same analysis window as in A). Only positive correlations are shown. " /><figcaption aria-hidden="true"> <strong>Figure 4: Grand-average comparison between brain and model.</strong> Based on the response patterns to each stimulus type, three processing stages were identified that correspond to different time windows in the <span>MEG</span> activity and different layer types in the model.<br />
<strong>A:</strong> Grand-average response patterns obtained from evoked <span>MEG</span> activity, quantified using three <span>ECD</span> groups. For each stimulus, the <span>ECD</span> time courses were integrated over the indicated time interval.<br />
<strong>B:</strong> Response patterns obtained from the model. For each layer, the mean <span>ReLu</span> activation in response to each stimulus. Matching response patterns between <span>ECDs</span> and model layers are shown using black lines, accompanied by the corresponding Pearson correlations. Two additional correlation values are shown in gray, for which the response patterns do not match, but which are discussed in the main text.<br />
<strong>C:</strong> Pearson correlation between the model and the <span data-acronym-label="MNE-dSPM" data-acronym-form="singular+short">MNE-dSPM</span> source estimate. Grand-average source estimates were obtained in response to each stimulus. The correlation map was obtained by correlating the activity at each source point with that for each layer of the model. The correlation map is shown at the time of peak correlation (using the same analysis window as in <strong>A</strong>). Only positive correlations are shown. </figcaption>
</figure>
<div id="tab:correlations">
<table>
<caption> <strong>Table 1: Single-participant comparison between brain and model.</strong> Shown are the average Pearson correlations between the single-participant <span>ECD</span> response patterns and the response patterns for each layer of the model. </caption>
<thead>
<tr class="header">
<th style="text-align: right;"><span>Layer</span></th>
<th style="text-align: left;"><span>Type</span></th>
<th style="text-align: center;"><span>Type I</span></th>
<th style="text-align: center;"><span>Type II</span></th>
<th style="text-align: center;"><span>N400m</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: left;">Convolution</td>
<td style="text-align: center;">0.33*</td>
<td style="text-align: center;">-0.15</td>
<td style="text-align: center;">-0.13***</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: left;">Convolution</td>
<td style="text-align: center;">0.33*</td>
<td style="text-align: center;">-0.15</td>
<td style="text-align: center;">-0.13***</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: left;">Convolution</td>
<td style="text-align: center;">0.32*</td>
<td style="text-align: center;">-0.15</td>
<td style="text-align: center;">-0.12***</td>
</tr>
<tr class="even">
<td style="text-align: right;">4</td>
<td style="text-align: left;">Convolution</td>
<td style="text-align: center;">0.25*</td>
<td style="text-align: center;">-0.10</td>
<td style="text-align: center;">-0.08***</td>
</tr>
<tr class="odd">
<td style="text-align: right;">5</td>
<td style="text-align: left;">Convolution</td>
<td style="text-align: center;">0.29*</td>
<td style="text-align: center;">-0.11</td>
<td style="text-align: center;">-0.08***</td>
</tr>
<tr class="even">
<td style="text-align: right;">6</td>
<td style="text-align: left;">Fully connected</td>
<td style="text-align: center;">-0.13*</td>
<td style="text-align: center;">0.12**</td>
<td style="text-align: center;">0.14***</td>
</tr>
<tr class="odd">
<td style="text-align: right;">7</td>
<td style="text-align: left;">Fully connected</td>
<td style="text-align: center;">-0.22*</td>
<td style="text-align: center;">0.15*</td>
<td style="text-align: center;">0.15***</td>
</tr>
<tr class="even">
<td style="text-align: right;">8</td>
<td style="text-align: left;">Output</td>
<td style="text-align: center;">-0.26*</td>
<td style="text-align: center;">0.15*</td>
<td style="text-align: center;">0.16***</td>
</tr>
</tbody>
</table>
</div>
<p><br />
<span><span class="math inline"><em>p</em> &lt; 0.05</span>  **<span class="math inline"><em>p</em> &lt; 0.01</span>  ***<span class="math inline"><em>p</em> &lt; 0.001</span><br />
<span>linear mixed effects (LME)</span> model, <span>false discovery rate (FDR)</span> corrected</span></p>
<p>In the first three convolution layers of the model, we see a much larger response to the noise-embedded word stimuli, relative to the other stimulus types. This is likely driven by the much higher visual complexity of the noisy stimuli. This corresponds well to the Type I response pattern, i.e. high response to noisy words and no distinction among the other stimulus types (correlations between grand-average patterns: 0.79/0.79/0.76, average single-participant correlations: 0.33/0.33/0.32 [<span class="math inline"><em>p</em> &lt; 0.05</span>]).</p>
<p>We see a change in the response pattern of the model in the next two convolution layers, layers 4 and 5. In these layers, the <span>ReLu</span> activation for symbol strings is now lower than that for the other stimulus types. This indicates the presence of convolution filters that are sensitive to the specific line arrangements that are found in letters of the Finnish alphabet, but not present in the symbol strings. In this regard, the response pattern of these layers is similar to the Type II response pattern. However, many of the filters remain sensitive to noise, which does not match the Type II response pattern, resulting in a negative correlation (correlations between grand-average patterns: -0.30/-0.34, average single-participant correlations: -0.10/-0.11 [n.s.]). Overall, the response patterns of layers 4 and 5 correlate best with the Type I response pattern, although they are not strictly consistent (correlations between grand-average patterns: 0.60/0.69, averaged single-trial correlations: 0.25/0.29 [<span class="math inline"><em>p</em> &lt; 0.05</span>]).</p>
<p>The selectivity for letter shapes becomes more pronounced in the two fully connected layers of the model. In these layers, we see an increase in <span>ReLu</span> activation for stimuli containing letter shapes, and a decrease in activation for noise-embedded words. However, no distinction is made between consonant strings and word-like stimuli, indicating that letters are detected in isolation. This means the response patterns of these layers are consistent with the Type II response pattern (correlations with grand-average: 0.39/0.47, averaged subject-subject correlations: 0.12 [<span class="math inline"><em>p</em> &lt; 0.01</span>], 0.15 [<span class="math inline"><em>p</em> &lt; 0.05</span>]), and is consistent with the Type II response.</p>
<p>Finally, at the output layer of the model, we observe that word-like stimuli produce more activity than consonant strings, indicating that the shapes of multiple letters are combined to produce the output. It is noteworthy that pseudowords, which were not present in the training set, produce a roughly equal amount of activation in the output layer as valid Finnish words. This means that the response pattern of the output layer corresponds best to that of the third <span>ECD</span> group in the temporal cortex (correlation with grand-average: 0.44, average subject-subject correlation: 0.16 [<span class="math inline"><em>p</em> &lt; 0.001</span>]), and is consistent with the N400m response<span class="citation" data-cites="Salmelin2007"> (Salmelin 2007)</span>.</p>
<p>To verify that the three <span>ECD</span> groups capture the most important correlations between the brain activity and the model, we compared the model to cortex-wide <span data-acronym-label="MNE-dSPM" data-acronym-form="singular+short">MNE-dSPM</span> source estimates (Figure 4C). For each layer in the model, the areas with maximum correlation appear around the locations of the <span>ECDs</span> and at a time that is close to the peak activation of the <span>ECDs</span>.</p>
<h1 id="discussion">Discussion</h1>
<p>In this study, we built a model of visual word recognition that operates on the raw pixel values of a bitmap image, rather than the more abstract representations found in many earlier models<span class="citation" data-cites="McClelland1981 Coltheart2001 Laszlo2014"> (McClelland and Rumelhart 1981; Coltheart et al. 2001; Laszlo and Armstrong 2014)</span>. Such a model can be tested with a wider variety of visual stimuli than traditional models, including a more naturalistic range of fonts, sizes, rotations, as well as non-letter stimuli such as visual noise and symbol strings. Furthermore, the deep-learning architecture scales to encompass a vocabulary with the same order of magnitude as a human’s, enabling it to be tested meaningfully with pseudowords. The combination of all of this means that the model is able to operate on identical stimuli as those commonly used in neuroimaging studies of reading, and is able to perform the same task as the volunteer, with comparable accuracy and robustness against irrelevant visual detail. This bridges an important gap between modeling and experimental efforts in this domain, namely the ability to evaluate a model by its numerical fit to neuroimaging data.</p>
<p>Our model aims to provide a good fit to the brain activity observed during the first 400 ms after stimulus onset. Specifically, the mean <span>ReLu</span> activity in the layers of the model provides a good explanation of the responses of three important evoked components that are observed during <span>MEG</span> studies of word recognition<span class="citation" data-cites="Salmelin2007"> (Salmelin 2007)</span>. Specifically, we observed that the convolution layers respond more to noise embedded words than the stimuli without noise, which mimics the Type I response that is observed around 64–115 ms in the visual cortex. Next, two fully connected layers responded more to valid Finnish letters than letter-like symbol strings, which mimics the Type II response that is observed around 140–200 ms occipital-temporally in the brain<span class="citation" data-cites="Tarkiainen1999"> (Tarkiainen et al. 1999)</span>. Finally, the output layer responded more to words and pseudo-words than consonant strings, which mimics the N400m response that is observed around 300–500 ms in the temporal cortex<span class="citation" data-cites="Halgren2002 Helenius1998b Service2007 Salmelin2007"> (Halgren et al. 2002; Helenius et al. 1998; Service et al. 2007; Salmelin 2007)</span>. The progression of the signal through the layers in the model tracks the dynamics of the <span>MEG</span> data, with earlier layers matching with earlier time points.</p>
<p>The model uses a conventional <span>CNN</span> architecture that is similar to architectures that have previously been shown to describe the initial stages of visual object recognition<span class="citation" data-cites="Schrimpf2018 Devereux2018 Yamins2016"> (Schrimpf et al. 2018; Devereux, Clarke, and Tyler 2018; Yamins and DiCarlo 2016)</span>. No changes to the <span class="smallcaps">vgg</span>-11 architecture<span class="citation" data-cites="Simonyan2015"> (Simonyan and Zisserman 2015)</span> were needed to create a successful model. Using an approach that somewhat mimics how humans build an orthographic system on top of their existing visual system when they learn how to read<span class="citation" data-cites="Vinckier2007 Cohen2004"> (Vinckier et al. 2007; Cohen and Dehaene 2004)</span>, we started with a model that was pre-trained to perform visual object recognition on ImageNet, and then further trained it using bitmap images of written words. The resulting model learned to recognize letter shapes and combine them into one of 10 000 possible words.</p>
<p>During training, the learning algorithm was required to adapt the existing visual network into a model of visual word recognition. This is in line with the observation that proficiency with written language has developed too recently in our history for any evolutionary changes in brain connectivity to support this skill to have occurred. Instead, acquisition of rapid visual word recognition likely relies on a reconfiguration of parts of an existing visual network<span class="citation" data-cites="Dehaene2018"> (Dehaene-Lambertz, Monzalvo, and Dehaene 2018)</span>.</p>
<p>Comparing unit activations in a <span>CNN</span> to <span>MEG</span> data means comparing two high-dimensional datasets, for which a mapping from one unto the other needs to be created. In other studies, this was often achieved through information-based techniques such as <span>representational similarity analysis (RSA)</span> or pattern-based techniques such as multivariate regression. Such methods allow for a lot of flexibility in how the model maps onto the brain signal. However, this flexibility is ultimately undesired when we strive for a more complete understanding of how computational processes in the brain manifest in the signal recorded by the scanner. In this study, we were able to make the mapping between the model and brain activity explicit enough to enable a rigid univariate comparison.</p>
<h2 id="limitations-of-the-current-model">Limitations of the current model</h2>
<p>The <span class="smallcaps">vgg-11</span> architecture was originally designed to achieve high image classification accuracy on the ImageNet challenge<span class="citation" data-cites="Simonyan2015"> (Simonyan and Zisserman 2015)</span>. As a model of brain function to explain neuroimaging data during reading, it is still incomplete in many ways. As neuroimaging experiments of language started by mapping responses to basic visual and auditory stimuli, so does the modeling effort start with an implementation of only the most basic principles.</p>
<p>One important limitation of the current model is the lack of an explicit mapping from the units inside its layers to specific locations in the brain at specific times. Multiple layers have a maximum correlation with the source estimate at the exact same location and time (Figure 4C). Furthermore, there is no clear relationship between the number of layers the signal needs to traverse in the model to the processing time in the brain. The signal needs to propagate through five convolution layers to reach the point where it matches the Type II response at 140–200 ms, but only through two additional layers to reach the point where it matches the N400m response at 300–500 ms. There are network architectures like <span class="smallcaps">corn</span>et-<span class="smallcaps">s</span><span class="citation" data-cites="Kubilius2019a"> (Kubilius et al. 2019)</span> that aim for an explicit mapping between layers in the model and the V1-V4 areas of the visual cortex, although the first layer of the model should probably be mapped onto the retina, rather than V1<span class="citation" data-cites="Lindsay2019"> (Lindsey et al. 2019)</span>.</p>
<p>As for all models that rely on machine learning to set millions of parameters, it’s not always straightforward to understand what exact computations the model itself is performing. In this study, we used Deep<span class="smallcaps">lift</span> contribution maps to gain some insight into this (Figure 3), and others have explored this issue more deeply in the specific context of <span>CNNs</span> that perform visual word recognition<span class="citation" data-cites="Testolin2017 Hannagan2021"> (Testolin, Stoianov, and Zorzi 2017; Hannagan et al. 2021)</span>.</p>
<p>Furthermore, the scope of the current model is limited to a bottom-up processing stream that starts from the initial pixel input up to recognition of a single orthographic word-form up until right before the formation of any abstract semantic representations. However, there is some basis for an extension of the model into the semantic domain, as there is a growing body of literature exploring how to learn suitable representations from text corpora<span class="citation" data-cites="Gunther2019 Mikolov2013 Pennington2014"> (Günther, Rinaldi, and Marelli 2019; Mikolov et al. 2013; Pennington, Socher, and Manning 2014)</span> and how to combine multiple words into sentence representations<span class="citation" data-cites="Brown2020 Devlin2019"> (Brown et al. 2020; Devlin et al. 2019)</span>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Despite its limitations, our model is an important milestone for computational implementations of cognitive theories of language on the level of the brain. The ability to directly compare a model with neuroimaging results greatly facilitates empirical verifiability of such theories. Our model is a powerful platform for future extensions that implement more accurate, detailed and complete theories, where the merit of such extensions can be evaluated based on the improvement of the fit to the data.</p>
<h1 id="methods">Methods</h1>
<h2 id="meg-study-design-and-data-analysis">MEG study design and data analysis</h2>
<p>The <span>MEG</span> data was collected by <span class="citation" data-cites="Vartiainen2011">Vartiainen et al. (2011)</span>, and was re-analyzed for the current study using <span class="smallcaps">MNE</span>-python<span class="citation" data-cites="Gramfort2013"> (Gramfort et al. 2013)</span> and FreeSurfer<span class="citation" data-cites="Dale1999"> (Dale, Fischl, and Sereno 1999)</span>. We refer to the original publication for additional details on the study protocol and data collection process.</p>
<p>Simultaneous <span>MEG</span> and <span>EEG</span> was recorded from 15 participants (who gave their informed consent, in agreement with the prior approval of the Ethics Committee of the Helsinki and Uusimaa Hospital District). The stimuli consisted of visually presented Finnish words, pseudowords (pronounceable but meaningless), consonant strings (random consonants), symbol strings (randomly selected from 10 possible shapes), and words embedded in high-frequency visual noise (Figure 1A). Each stimulus category contained 112 different stimuli and each stimulus contained 7–8 letters/symbols. The stimuli were presented sequentially in a block design. Each block started with a random period of rest (0–600 ms), followed by 7 stimuli of the same category. Each stimulus was shown for 300 ms with an inter-stimulus interval of 1500 ms of gray background. In addition to the five stimulus conditions (16 blocks each), there were 16 blocks of rest (12 s) An additional 5 target blocks were added, in wich one stimulus appeared twice in a row, that the participants were asked to detect and respond with a button press. The target blocks were not included in the analysis. The same paradigm was applied in an fMRI-EEG recording of the same participants as well (data not used here).</p>
<p>The data were recorded with a Vector View device (Elekta Neuromag, now MEGIN Oy, Finland) in a magnetically shielded room. In addition vertical and horizontal <span>electro-oculography (EOG)</span> were recorded. The signals were bandpass filtered at 0.03–200 Hz and digitized at 600 Hz. For analysis, the data were further processed using the maxfilter software (MEGIN Oy, Finland) and bandpass filtered at 0.1–40 Hz. Eye-movement and heart-beat artifacts were reduced using <span>independent component analysis (ICA)</span>: the signal was highpassed at 1 Hz, and components with correlations larger than 5 standard deviations with any of the <span>EOG</span> channels or an estimated electro-cardiogram (based on the magnetometers without the maxfilter applied) were removed (3–7 out of 86–108 components). Epochs were cut −200–1000 ms relative to the onset of the stimulus and baseline-corrected using the pre-stimulus interval. Participant-specific noise covariance matrices of the <span>MEG</span> sensors were estimated using the pre-stimulus interval.</p>
<p>For <span>ECD</span> and <span data-acronym-label="MNE-dSPM" data-acronym-form="singular+short">MNE-dSPM</span> source estimation, 3-layer <span>boundary element method (BEM)</span> forward models were used. These models were based on T1-weighted anatomical images that were acquired using a 3 T Signa <span class="smallcaps">excite</span> <span>magnetic resonance imaging (MRI)</span> scanner. FreeSurfer<span class="citation" data-cites="Dale1999"> (Dale, Fischl, and Sereno 1999)</span> was used to obtain the surface reconstructions required to compute the <span>BEM</span> models. <span class="citation" data-cites="Vartiainen2011">Vartiainen et al. (2011)</span> defines three groups of <span>ECDs</span> in the left-hemisphere, associated with the Type I, Type II and N400m responses. The locations and orientations of the <span>ECDs</span> in these groups were re-used in the current study. For each individual epoch, the signal at each of the three <span>ECDs</span> and a cortex-wide <span data-acronym-label="MNE-dSPM" data-acronym-form="singular+short">MNE-dSPM</span> source estimate was computed using the noise covariance matrices and <span>BEM</span> forward models. Finally, the locations of the <span>ECDs</span> and <span data-acronym-label="MNE-dSPM" data-acronym-form="singular+short">MNE-dSPM</span> source points were morphed to a template brain (FreeSurfer’s fsaverage brain).</p>
<h2 id="model">Model</h2>
<p>The computational model has a <span class="smallcaps">vgg</span>-11 architecture<span class="citation" data-cites="Simonyan2015"> (Simonyan and Zisserman 2015)</span> as defined in the TorchVision<span class="citation" data-cites="Marcel2010"> (Marcel and Rodriguez 2010)</span> module of the PyTorch<span class="citation" data-cites="Paszke2019"> (Paszke et al. 2019)</span> (Figure 2A). The architecture specifies 5 convolution layers, with the final 3 performing convolution twice. Each individual convolution step is followed by batch-normalization<span class="citation" data-cites="Ioffe2015"> (Ioffe and Szegedy 2015)</span> and <span>ReLu</span> activation, and max-pooling is performed as final step of each layer. The 5 convolution layers are followed by 2 followed by 2 linear fully-connected layers, with <span>ReLu</span> activation, and employing 50 % dropout. The final output layer is also a linear, fully-connected layer with <span>ReLu</span> activation.</p>
<p>The model parameters for all layers, except the output layer, were initialized to the ImageNet pre-trained model provided by TorchVision. Then, the model was trained using 1 000 000 bitmap images of rendered text on a background of visual noise, and 500 000 bitmap images containing only visual noise (Figure 2B). An independent test set containing 100 000 images of text and 50 000 images of only noise was used to track the performance of the model during training. None of the images in the training or test set were an exact duplicate of the images used as stimuli in the <span>MEG</span> experiment. Therefore, the stimulus set can be treated as an independent validation set.</p>
<p>The text on the images were one of 10 000 Finnish words. The list of words was compiled using the 112 words used in the <span>MEG</span>, extended with the most frequently used Finnish words, as determined by <span class="citation" data-cites="Kanerva2014">Kanerva et al. (2014)</span>, excluding proper names and using their lemma form. Words had a minimum length of 2 letters, and no preset maximum length. A training image was assembled by overlaying text onto an image of visual noise. Visual noise was rendered by randomly selecting a gray-scale value (0–100 %) for each pixel. Next, if the training image contained text, the word was rendered in uppercase letters, using a randomly chosen font (15 possible fonts), font size (14–32 pt), and rotation (-20<span>°</span> to 20<span>°</span>). The list of fonts consisted of: Ubuntu Mono, Courier, Luxi Mono Regular, Lucida Console, Lekton, Dejavu Sans Mono, Times New Roman, Arial, Arial Black, Verdana, Comic Sans MS, Georgia, Liberation Serif, Impact, and Roboto Condensed. The text and the visual noise were mixed at a ratio dictated by a randomly selected noise level (10–30 %) by adjusting their respective opacities to add up to 100 %. For example, at a noise level of 20 %, first the visual noise was rendered at 20 % opacity, then the text was overlayed at <span class="math inline">100 − 20 = 80 %</span> opacity. 0 When the input image did not contain a word, all model outputs should be low, which was achieved by adding a unit to the output layer to indicate this condition. The training objective was to minimize the cross-entropy loss between the model output and a one-hot encoded vector of length 10 001, indicating the target word or the no-word condition. After training, the extra unit was removed from the model and a <span>ReLu</span> activation was placed on the remaining units. Training was performed using stochastic gradient descend with an initial learning rate of 0.001, which was reduced to 0.0001 after 10 epochs (an "epoch" in this context refers to one sequence of all 1 500 000 training images). In total, training was performed for 20 epochs, by which point the performance on the test set had plateaued at 99.6 %.</p>
<h2 id="comparison-between-model-and-meg-data">Comparison between model and MEG data</h2>
<p>To compare the model and <span>MEG</span>, statistical summaries were made of the high-dimensional datasets, such that Pearson correlations could be computed between them. We will refer to these summaries as "activity scores".</p>
<p>After training, the model was applied to the bitmap images that were used during the <span>MEG</span> experiment (padding them to be <span class="math inline">256 × 256</span> pixels with 50 % greyscale pixels). The activity at each layer of the model was quantified by recording the <span>ReLu</span> activation right before the maxpool operation, and computing the mean across the units. This yielded for each stimulus image, a single number for each layer in the model representing the amount of activity within that layer. For each layer, the activity scores were z-scored (subtract the mean, divide by the standard deviation) across the stimulus images (Figure 4B).</p>
<p>The activity of each <span>ECD</span> was quantified by taking the mean of the <span>ECD</span>’s timecourse (Figure 1C) during the time window used in <span class="citation" data-cites="Vartiainen2011">Vartiainen et al. (2011)</span>. This yielded, for each stimulus image, a single number for each <span>ECD</span> for each participant, indicating the amount of signal at the <span>ECD</span>. These activity scores were first z-scored across the stimulus images independently for each participant. The <span>ECDs</span> are participant specific. <span class="citation" data-cites="Vartiainen2011">Vartiainen et al. (2011)</span> clustered the <span>ECDs</span> into groups using their location and timing, where the <span>ECDs</span> in each group were judged to encode the same evoked component in different participants. Three groups were selected as representing the Type I, Type II and N400m responses typically found in the literature<span class="citation" data-cites="Salmelin2007"> (Salmelin 2007)</span>. The groups do not necessarily contain an <span>ECD</span> for each of the 15 participants, as some participants did not have an <span>ECD</span> to contribute that matched well enough in position and/or timing. The number of participants contributing an <span>ECD</span> to each group were: Type I-14, Type II-14, and N400m-15. During the grand-average analysis, the activity score of the <span>ECDs</span> was averaged within each group (Figure 4A).</p>
<p>The <span data-acronym-label="MNE-dSPM" data-acronym-form="singular+short">MNE-dSPM</span> source estimates used 4000 source points, distributed evenly across the cortex. The activity score at each source point was computed by taking the maximum signal value of the source point’s timecourse within the same time window as the <span>ECD</span> analysis. This yielded for each stimulus image, a single number for each source point for each participant, indicating the amount of signal at the source point (Figure 1C). These activity scores were first z-scored across the stimulus images independently for each participant. Then, during the grand-average analysis, the scores were averaged across participants.</p>
<p>In the grand-average comparison, Pearson correlations were computed between the activity scores of each layer in the model versus each group of <span>ECDs</span> (annotated in Figure 4B) and each <span data-acronym-label="MNE-dSPM" data-acronym-form="singular+short">MNE-dSPM</span> source point (Figure 4C). In the single-participant comparison, a separate <span>LME</span> model was fitted for each layer in the model versus each group of <span>ECDs</span>, using participants as random effect (random intercept + random slope). The lme4<span class="citation" data-cites="Bates2015"> (Bates et al. 2015)</span> R-package implements the <span>LME</span> model and lmeTest<span class="citation" data-cites="Kuznetsova2015"> (Kuznetsova, Brockhoff, and Christensen 2015)</span> was used to estimate <span class="math inline"><em>p</em></span>-values.</p>
<h2 id="data-availability">Data availability</h2>
<p>The trained model and <span>MEG</span> data necessary for performing the comparison is available at <a href="https://osf.io/nu2ep">https://osf.io/nu2ep</a>. As the consent obtained from the participants prohibits sharing of private data, the <span>MEG</span> data that is made available has been processed to such a degree that participant anonymity can be safeguarded. The code for training the model and reproducing the results of this paper is available at <a href="https://github.com/wmvanvliet/viswordrec-baseline">https://github.com/wmvanvliet/viswordrec-baseline</a>.</p>
<h1 id="acknowledgements">Acknowledgements</h1>
<p>We acknowledge the computational resources provided by the Aalto Science-IT project. This research was funded by the Academy of Finland (grants #310988 and #343385 to M.v.V, #315553 to R.S.) and the Sigrid Jesélius Foundation (to R.S.).</p>
<h1 id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Barber2007" class="csl-entry" role="doc-biblioentry">
Barber, Horacio A., and Marta Kutas. 2007. <span>“<span class="nocase">Interplay between computational models and cognitive electrophysiology in visual word recognition</span>.”</span> <em>Brain Res. Rev.</em> 53 (1): 98–123. <a href="https://doi.org/10.1016/j.brainresrev.2006.07.002">https://doi.org/10.1016/j.brainresrev.2006.07.002</a>.
</div>
<div id="ref-Bates2015" class="csl-entry" role="doc-biblioentry">
Bates, Douglas M., Martin Maechler, Benjamin M Bolker, and Steven Walker. 2015. <span>“Fitting <span>Linear Mixed-Effects Models</span> Using Lme4.”</span> <em>Journal Of Statistical Software</em> 67 (1): 1–48. <a href="https://doi.org/10.18637/jss.v067.i01">https://doi.org/10.18637/jss.v067.i01</a>.
</div>
<div id="ref-Brown2020" class="csl-entry" role="doc-biblioentry">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> <em>arXiv:2005.14165 [Cs]</em>. <a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a>.
</div>
<div id="ref-Carreiras2014" class="csl-entry" role="doc-biblioentry">
Carreiras, Manuel, Blair C. Armstrong, Manuel Perea, and Ram Frost. 2014. <span>“The What, When, Where, and How of Visual Word Recognition.”</span> <em>Trends in Cognitive Sciences</em> 18 (2): 90–98. <a href="https://doi.org/10.1016/j.tics.2013.11.005">https://doi.org/10.1016/j.tics.2013.11.005</a>.
</div>
<div id="ref-Cohen2004" class="csl-entry" role="doc-biblioentry">
Cohen, Laurent, and Stanislas Dehaene. 2004. <span>“Specialization Within the Ventral Stream: The Case for the Visual Word Form Area.”</span> <em>NeuroImage</em> 22 (1): 466–76. <a href="https://doi.org/10.1016/j.neuroimage.2003.12.049">https://doi.org/10.1016/j.neuroimage.2003.12.049</a>.
</div>
<div id="ref-Coltheart2001" class="csl-entry" role="doc-biblioentry">
Coltheart, M, K Rastle, Conrad Perry, R Langdon, and J Ziegler. 2001. <span>“<span>DRC</span>: A Dual Route Cascaded Model of Visual Word Recognition and Reading Aloud.”</span> <em>Psychological Review</em> 108 (1): 204–56. <a href="https://doi.org/10.1037/0033-295X.108.1.204">https://doi.org/10.1037/0033-295X.108.1.204</a>.
</div>
<div id="ref-Dai2021" class="csl-entry" role="doc-biblioentry">
Dai, Zihang, Hanxiao Liu, Quoc V. Le, and Mingxing Tan. 2021. <span>“<span>CoAtNet</span>: <span>Marrying Convolution</span> and <span>Attention</span> for <span>All Data Sizes</span>.”</span> <em>arXiv:2106.04803 [Cs]</em>. <a href="https://arxiv.org/abs/2106.04803">https://arxiv.org/abs/2106.04803</a>.
</div>
<div id="ref-Dale1999" class="csl-entry" role="doc-biblioentry">
Dale, Anders M., Bruce R. Fischl, and Martin I. Sereno. 1999. <span>“Cortical Surface-Based Analysis: <span>I</span>. Segmentation and Surface Reconstruction.”</span> <em>NeuroImage</em> 9 (2): 179–94.
</div>
<div id="ref-Dehaene2018" class="csl-entry" role="doc-biblioentry">
Dehaene-Lambertz, Ghislaine, Karla Monzalvo, and Stanislas Dehaene. 2018. <span>“The Emergence of the Visual Word Form: Longitudinal Evolution of Category-Specific Ventral Visual Areas During Reading Acquisition.”</span> <em>PLOS Biology</em> 16 (3): e2004103. <a href="https://doi.org/10.1371/journal.pbio.2004103">https://doi.org/10.1371/journal.pbio.2004103</a>.
</div>
<div id="ref-Devereux2018" class="csl-entry" role="doc-biblioentry">
Devereux, Barry J., Alex Clarke, and Lorraine K. Tyler. 2018. <span>“Integrated Deep Visual and Semantic Attractor Neural Networks Predict <span class="nocase">fMRI</span> Pattern-Information Along the Ventral Object Processing Pathway.”</span> <em>Scientific Reports</em> 8 (1). <a href="https://doi.org/10.1038/s41598-018-28865-1">https://doi.org/10.1038/s41598-018-28865-1</a>.
</div>
<div id="ref-Devlin2019" class="csl-entry" role="doc-biblioentry">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <span>“<span>BERT</span>: Pre-Training of Deep Bidirectional Transformers for Language Understanding.”</span> <em>arXiv:1810.04805 [Cs]</em>. <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a>.
</div>
<div id="ref-Grainger2009" class="csl-entry" role="doc-biblioentry">
Grainger, Jonathan, and Phillip J. Holcomb. 2009. <span>“Watching the Word Go by: On the Time-Course of Component Processes in Visual Word Recognition.”</span> <em>Linguistics and Language Compass</em> 3 (1): 128–56. <a href="https://doi.org/10.1111/j.1749-818X.2008.00121.x">https://doi.org/10.1111/j.1749-818X.2008.00121.x</a>.
</div>
<div id="ref-Gramfort2013" class="csl-entry" role="doc-biblioentry">
Gramfort, Alexandre, Martin Luessi, Eric Larson, Denis A. Engemann, Daniel Strohmeier, Christian Brodbeck, Roman Goj, et al. 2013. <span>“<span>MEG</span> and <span>EEG</span> Data Analysis with <span>MNE-Python</span>.”</span> <em>Frontiers in Neuroscience</em> 7 (December): 1–13. <a href="https://doi.org/10.3389/fnins.2013.00267">https://doi.org/10.3389/fnins.2013.00267</a>.
</div>
<div id="ref-Gunther2019" class="csl-entry" role="doc-biblioentry">
Günther, Fritz, Luca Rinaldi, and Marco Marelli. 2019. <span>“Vector-Space Models of Semantic Representation from a Cognitive Perspective: A Discussion of Common Misconceptions.”</span> <em>Perspectives on Psychological Science</em> 14 (6): 1006–33. <a href="https://doi.org/10.1177/1745691619861372">https://doi.org/10.1177/1745691619861372</a>.
</div>
<div id="ref-Halgren2002" class="csl-entry" role="doc-biblioentry">
Halgren, Eric, Rupali P. Dhond, Natalie Christensen, Cyma Van Petten, Ksenija Marinkovic, Jeffrey D. Lewine, and Anders M. Dale. 2002. <span>“N400-Like Magnetoencephalography Responses Modulated by Semantic Context, Word Frequency, and Lexical Class in Sentences.”</span> <em>NeuroImage</em> 17 (3): 1101–16. <a href="https://doi.org/10.1006/nimg.2002.1268">https://doi.org/10.1006/nimg.2002.1268</a>.
</div>
<div id="ref-Hannagan2021" class="csl-entry" role="doc-biblioentry">
Hannagan, T., A. Agrawal, L. Cohen, and S. Dehaene. 2021. <span>“Emergence of a Compositional Neural Code for Written Words: Recycling of a Convolutional Neural Network for Reading.”</span> <em>Proceedings of the National Academy of Sciences</em> 118 (46). <a href="https://doi.org/10.1073/pnas.2104779118">https://doi.org/10.1073/pnas.2104779118</a>.
</div>
<div id="ref-Hansen2010" class="csl-entry" role="doc-biblioentry">
Hansen, Peter, Morten Kringelbach, and Riitta Salmelin, eds. 2010. <em><span>MEG</span>: An Introduction to Methods</em>. <span>New York</span>: <span>Oxford University Press</span>. <a href="https://doi.org/10.1093/acprof:oso/9780195307238.001.0001">https://doi.org/10.1093/acprof:oso/9780195307238.001.0001</a>.
</div>
<div id="ref-Helenius1998b" class="csl-entry" role="doc-biblioentry">
Helenius, P., R. Salmelin, E. Service, and J. F. Connolly. 1998. <span>“Distinct Time Courses of Word and Context Comprehension in the Left Temporal Cortex.”</span> <em>Brain</em> 121 (6): 1133–42. <a href="https://doi.org/10.1093/brain/121.6.1133">https://doi.org/10.1093/brain/121.6.1133</a>.
</div>
<div id="ref-Ioffe2015" class="csl-entry" role="doc-biblioentry">
Ioffe, Sergey, and Christian Szegedy. 2015. <span>“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.”</span> In <em>Proceedings of the 32nd <span>International Conference</span> on <span>International Conference</span> on <span>Machine Learning</span> - <span>Volume</span> 37</em>, 448–56. <span>ICML</span>’15. <span>JMLR.org</span>.
</div>
<div id="ref-Jobard2003" class="csl-entry" role="doc-biblioentry">
Jobard, G, F Crivello, and N Tzourio-Mazoyer. 2003. <span>“Evaluation of the Dual Route Theory of Reading: A Metanalysis of 35 Neuroimaging Studies.”</span> <em>NeuroImage</em> 20 (2): 693–712. <a href="https://doi.org/10.1016/S1053-8119(03)00343-4">https://doi.org/10.1016/S1053-8119(03)00343-4</a>.
</div>
<div id="ref-Kanerva2014" class="csl-entry" role="doc-biblioentry">
Kanerva, Jenna, Juhani Luotolahti, Veronika Laippala, and Filip Ginter. 2014. <span>“Syntactic <span class="nocase">N-gram Collection</span> from a <span>Large-Scale Corpus</span> of <span>Internet Finnish</span>.”</span> <em>Frontiers in Artificial Intelligence and Applications</em> 268: 184–91. <a href="https://doi.org/10.3233/978-1-61499-442-8-184">https://doi.org/10.3233/978-1-61499-442-8-184</a>.
</div>
<div id="ref-Krizhevsky2017" class="csl-entry" role="doc-biblioentry">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. <span>“<span>ImageNet</span> Classification with Deep Convolutional Neural Networks.”</span> <em>Communications of the ACM</em> 60 (6): 84–90. <a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a>.
</div>
<div id="ref-Kubilius2019a" class="csl-entry" role="doc-biblioentry">
Kubilius, Jonas, Martin Schrimpf, Kohitij Kar, Rishi Rajalingham, Ha Hong, Najib Majaj, Elias Issa, et al. 2019. <span>“Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs.”</span> In <em>Advances in <span>Neural Information Processing Systems</span></em>. Vol. 32. <span>Curran Associates, Inc.</span>
</div>
<div id="ref-Kutas2011" class="csl-entry" role="doc-biblioentry">
Kutas, Marta, and Kara D. Federmeier. 2011. <span>“<span class="nocase">Thirty years and counting: Finding meaning in the N400 component of the event related brain potential (ERP)</span>.”</span> <em>Annu. Rev. Psychol.</em> 62 (January): 621. <a href="https://doi.org/10.1146/annurev.psych.093008.131123">https://doi.org/10.1146/annurev.psych.093008.131123</a>.
</div>
<div id="ref-Kuznetsova2015" class="csl-entry" role="doc-biblioentry">
Kuznetsova, A., P. B. Brockhoff, and R. H. B. Christensen. 2015. <span>“<span>LmerTest</span>: <span>Tests</span> for Random and Fixed Effects for Linear Mixed Effect Models. <span>R</span> Package, Version 2.0-29.”</span>
</div>
<div id="ref-Laszlo2014" class="csl-entry" role="doc-biblioentry">
Laszlo, Sarah, and Blair C. Armstrong. 2014. <span>“<span class="nocase">PSPs and ERPs: Applying the dynamics of post-synaptic potentials to individual units in simulation of temporally extended Event-Related Potential reading data</span>.”</span> <em>Brain Lang.</em> 132: 22–27. <a href="https://doi.org/10.1016/j.bandl.2014.03.002">https://doi.org/10.1016/j.bandl.2014.03.002</a>.
</div>
<div id="ref-Laszlo2012" class="csl-entry" role="doc-biblioentry">
Laszlo, Sarah, and David C. Plaut. 2012. <span>“<span class="nocase">A neurally plausible Parallel Distributed Processing model of Event-Related Potential word reading data</span>.”</span> <em>Brain and Language</em> 120 (3): 271–81. <a href="https://doi.org/10.1016/j.bandl.2011.09.001">https://doi.org/10.1016/j.bandl.2011.09.001</a>.
</div>
<div id="ref-LeCun2015" class="csl-entry" role="doc-biblioentry">
LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. <span>“Deep Learning.”</span> <em>Nature</em> 521 (7553): 436–44. <a href="https://doi.org/10.1038/nature14539">https://doi.org/10.1038/nature14539</a>.
</div>
<div id="ref-Lindsay2020" class="csl-entry" role="doc-biblioentry">
Lindsay, Grace W. 2020. <span>“Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future.”</span> <em>Journal of Cognitive Neuroscience</em>, 1–15. <a href="https://doi.org/10.1162/jocn_a_01544">https://doi.org/10.1162/jocn_a_01544</a>.
</div>
<div id="ref-Lindsay2019" class="csl-entry" role="doc-biblioentry">
Lindsey, Jack, Samuel A. Ocko, Surya Ganguli, and Stephane Deny. 2019. <span>“A Unified Theory of Early Visual Representations from Retina to Cortex Through Anatomically Constrained Deep <span>CNNs</span>.”</span> In <em>International Conference on Learning Representations</em>. <a href="https://doi.org/10.1101/511535">https://doi.org/10.1101/511535</a>.
</div>
<div id="ref-Marcel2010" class="csl-entry" role="doc-biblioentry">
Marcel, Sébastien, and Yann Rodriguez. 2010. <span>“Torchvision the Machine-Vision Package of Torch.”</span> In <em>Proceedings of the 18th <span>ACM</span> International Conference on <span>Multimedia</span></em>, 1485–88. <span>MM</span> ’10. <span>Association for Computing Machinery</span>. <a href="https://doi.org/10.1145/1873951.1874254">https://doi.org/10.1145/1873951.1874254</a>.
</div>
<div id="ref-McClelland2003" class="csl-entry" role="doc-biblioentry">
McClelland, James L., and Timothy T. Rogers. 2003. <span>“<span class="nocase">The parallel distributed processing approach to semantic cognition.</span>”</span> <em>Nat. Rev. Neurosci.</em> 4 (4): 310–22. <a href="https://doi.org/10.1038/nrn1076">https://doi.org/10.1038/nrn1076</a>.
</div>
<div id="ref-McClelland1981" class="csl-entry" role="doc-biblioentry">
McClelland, James L., and David E. Rumelhart. 1981. <span>“An <span>Interactive Activation Model</span> of <span>Context Effects</span> in <span>Letter Perception</span>: <span>Part I</span>. <span>An Account</span> of <span>Basic Findings</span>.”</span> <em>Psychological Review</em> 88 (5): 580–96. <a href="https://doi.org/10.1016/B978-1-4832-1446-7.50048-0">https://doi.org/10.1016/B978-1-4832-1446-7.50048-0</a>.
</div>
<div id="ref-McLeod2000" class="csl-entry" role="doc-biblioentry">
McLeod, Peter, Tim Shallice, and David C. Plaut. 2000. <span>“<span class="nocase">Attractor dynamics in word recognition: Converging evidence from errors by normal subjects, dyslexic patients and a connectionist model</span>.”</span> <em>Cognition</em> 74 (1): 91–114. <a href="https://doi.org/10.1016/S0010-0277(99)00067-0">https://doi.org/10.1016/S0010-0277(99)00067-0</a>.
</div>
<div id="ref-Mikolov2013" class="csl-entry" role="doc-biblioentry">
Mikolov, Tomas, Greg Corrado, Kai Chen, and Jeffrey Dean. 2013. <span>“Efficient <span>Estimation</span> of <span>Word Representations</span> in <span>Vector Space</span>.”</span> <em>Proceedings of the International Conference on Learning Representations (ICLR 2013)</em>, 1–12. <a href="https://doi.org/10.1162/153244303322533223">https://doi.org/10.1162/153244303322533223</a>.
</div>
<div id="ref-Pammer2004" class="csl-entry" role="doc-biblioentry">
Pammer, Kristen, Peter C Hansen, Morten L Kringelbach, Ian Holliday, Gareth Barnes, Arjan Hillebrand, Krish D Singh, and Piers L Cornelissen. 2004. <span>“Visual Word Recognition: The First Half Second.”</span> <em>NeuroImage</em> 22 (4): 1819–25. <a href="https://doi.org/10.1016/j.neuroimage.2004.05.004">https://doi.org/10.1016/j.neuroimage.2004.05.004</a>.
</div>
<div id="ref-Paszke2019" class="csl-entry" role="doc-biblioentry">
Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, et al. 2019. <span>“PyTorch: An Imperative Style, High-Performance Deep Learning Library.”</span> In <em>Advances in Neural Information Processing Systems 32</em>, 8024–35. Curran Associates, Inc. <a href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</a>.
</div>
<div id="ref-Pennington2014" class="csl-entry" role="doc-biblioentry">
Pennington, Jeffrey, Richard Socher, and Christopher Manning. 2014. <span>“<span>GloVe</span>: Global <span>Vectors</span> for <span>Word Representation</span>.”</span> In <em>Proceedings of the 2014 <span>Conference</span> on <span>Empirical Methods</span> in <span>Natural Language Processing</span> (<span>EMNLP</span>)</em>, 1532–43. <span>Association for Computational Linguistics</span>. <a href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a>.
</div>
<div id="ref-Perry2007" class="csl-entry" role="doc-biblioentry">
Perry, Conrad, Johannes C. Ziegler, and Marco Zorzi. 2007. <span>“Nested Incremental Modeling in the Development of Computational Theories: The <span>CDP</span>+ Model of Reading Aloud.”</span> <em>Psychological Review</em> 114 (2): 273–315. <a href="https://doi.org/10.1037/0033-295X.114.2.273">https://doi.org/10.1037/0033-295X.114.2.273</a>.
</div>
<div id="ref-Petit2006" class="csl-entry" role="doc-biblioentry">
Petit, Jean-Philippe, Katherine J. Midgley, Phillip J. Holcomb, and Jonathan Grainger. 2006. <span>“On the Time Course of Letter Perception: <span>A</span> Masked Priming <span>ERP</span> Investigation.”</span> <em>Psychonomic Bulletin &amp; Review</em> 13 (4): 674–81. <a href="https://doi.org/10.3758/BF03193980">https://doi.org/10.3758/BF03193980</a>.
</div>
<div id="ref-Poeppel2012" class="csl-entry" role="doc-biblioentry">
Poeppel, David. 2012. <span>“<span class="nocase">The maps problem and the mapping problem: two challenges for a cognitive neuroscience of speech and language.</span>”</span> <em>Cognitive Neuropsycholy</em> 29 (1-2): 34–55. <a href="https://doi.org/10.1080/02643294.2012.710600">https://doi.org/10.1080/02643294.2012.710600</a>.
</div>
<div id="ref-Price2012" class="csl-entry" role="doc-biblioentry">
Price, Cathy J. 2012. <span>“A Review and Synthesis of the First 20years of <span>PET</span> and <span class="nocase">fMRI</span> Studies of Heard Speech, Spoken Language and Reading.”</span> <em>NeuroImage</em>, 20 <span class="nocase">YEARS OF fMRI</span>, 62 (2): 816–47. <a href="https://doi.org/10.1016/j.neuroimage.2012.04.062">https://doi.org/10.1016/j.neuroimage.2012.04.062</a>.
</div>
<div id="ref-Price2018" class="csl-entry" role="doc-biblioentry">
———. 2018. <span>“The Evolution of Cognitive Models: From Neuropsychology to Neuroimaging and Back.”</span> <em>Cortex</em> 107: 37–49. <a href="https://doi.org/10.1016/j.cortex.2017.12.020">https://doi.org/10.1016/j.cortex.2017.12.020</a>.
</div>
<div id="ref-Protopapas2016" class="csl-entry" role="doc-biblioentry">
Protopapas, Athanassios, Eleni Orfanidou, J. S. H. Taylor, Efstratios Karavasilis, Efthymia C. Kapnoula, Georgia Panagiotaropoulou, Georgios Velonakis, Loukia S. Poulou, Nikolaos Smyrnis, and Dimitrios Kelekis. 2016. <span>“Evaluating Cognitive Models of Visual Word Recognition Using <span class="nocase">fMRI</span>: <span>Effects</span> of Lexical and Sublexical Variables.”</span> <em>NeuroImage</em> 128: 328–41. <a href="https://doi.org/10.1016/j.neuroimage.2016.01.013">https://doi.org/10.1016/j.neuroimage.2016.01.013</a>.
</div>
<div id="ref-Richards2019" class="csl-entry" role="doc-biblioentry">
Richards, Blake A., Timothy P. Lillicrap, Philippe Beaudoin, Yoshua Bengio, Rafal Bogacz, Amelia Christensen, Claudia Clopath, et al. 2019. <span>“A Deep Learning Framework for Neuroscience.”</span> <em>Nature Neuroscience</em> 22 (11): 1761–70. <a href="https://doi.org/10.1038/s41593-019-0520-2">https://doi.org/10.1038/s41593-019-0520-2</a>.
</div>
<div id="ref-Rumelhart1982" class="csl-entry" role="doc-biblioentry">
Rumelhart, David E., and J L McClelland. 1982. <span>“An Interactive Activation Model of Context Effects in Letter Perception: <span>Part</span> 2. <span>The</span> Contextual Enhancement Effect and Some Tests and Extensions of the Model.”</span> <em>Psychological Review</em> 89 (1): 60–94. <a href="https://doi.org/10.1037/0033-295X.89.1.60">https://doi.org/10.1037/0033-295X.89.1.60</a>.
</div>
<div id="ref-Russakovsky2015" class="csl-entry" role="doc-biblioentry">
Russakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, et al. 2015. <span>“<span>ImageNet</span> Large Scale Visual Recognition Challenge.”</span> <em>International Journal of Computer Vision</em> 115 (3): 211–52. <a href="https://doi.org/10.1007/s11263-015-0816-y">https://doi.org/10.1007/s11263-015-0816-y</a>.
</div>
<div id="ref-Salmelin2007" class="csl-entry" role="doc-biblioentry">
Salmelin, Riitta. 2007. <span>“Clinical Neurophysiology of Language: <span>The MEG</span> Approach.”</span> <em>Clinical Neurophysiology</em> 118 (2): 237–54. <a href="https://doi.org/10.1016/j.clinph.2006.07.316">https://doi.org/10.1016/j.clinph.2006.07.316</a>.
</div>
<div id="ref-Schrimpf2018" class="csl-entry" role="doc-biblioentry">
Schrimpf, Martin, Jonas Kubilius, Ha Hong, Najib J. Majaj, Rishi Rajalingham, Elias B. Issa, Kohitij Kar, et al. 2018. <span>“Brain-<span>Score</span>: Which Artificial Neural Network for Object Recognition Is Most Brain-Like?”</span> <em>bioRxiv</em>, 407007. <a href="https://doi.org/10.1101/407007">https://doi.org/10.1101/407007</a>.
</div>
<div id="ref-Service2007" class="csl-entry" role="doc-biblioentry">
Service, Elisabet, Päivi Helenius, Sini Maury, and Riitta Salmelin. 2007. <span>“Localization of Syntactic and Semantic Brain Responses Using Magnetoencephalography.”</span> <em>Journal of Cognitive Neuroscience</em> 19 (7): 1193–1205. <a href="https://doi.org/10.1162/jocn.2007.19.7.1193">https://doi.org/10.1162/jocn.2007.19.7.1193</a>.
</div>
<div id="ref-Shrikumar2019" class="csl-entry" role="doc-biblioentry">
Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. 2019. <span>“Learning Important Features Through Propagating Activation Differences.”</span> <em>arXiv:1704.02685 [Cs]</em>. <a href="https://arxiv.org/abs/1704.02685">https://arxiv.org/abs/1704.02685</a>.
</div>
<div id="ref-Simonyan2015" class="csl-entry" role="doc-biblioentry">
Simonyan, Karen, and Andrew Zisserman. 2015. <span>“Very <span>Deep Convolutional Networks</span> for <span>Large</span>-<span>Scale Image Recognition</span>.”</span> <em>arXiv:1409.1556 [Cs]</em>. <a href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</a>.
</div>
<div id="ref-Szegedy2015" class="csl-entry" role="doc-biblioentry">
Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. <span>“Going Deeper with Convolutions.”</span> <em>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em> 07-12-June: 1–9. <a href="https://doi.org/10.1109/CVPR.2015.7298594">https://doi.org/10.1109/CVPR.2015.7298594</a>.
</div>
<div id="ref-Tarkiainen1999" class="csl-entry" role="doc-biblioentry">
Tarkiainen, A., P. Helenius, P. C. Hansen, P. L. Cornelissen, and R. Salmelin. 1999. <span>“Dynamics of Letter String Perception in the Human Occipitotemporal Cortex.”</span> <em>Brain</em> 122 (11): 2119–32. <a href="https://doi.org/10.1093/brain/122.11.2119">https://doi.org/10.1093/brain/122.11.2119</a>.
</div>
<div id="ref-Testolin2017" class="csl-entry" role="doc-biblioentry">
Testolin, Alberto, Ivilin Stoianov, and Marco Zorzi. 2017. <span>“Letter Perception Emerges from Unsupervised Deep Learning and Recycling of Natural Image Features.”</span> <em>Nature Human Behaviour</em> 1 (9): 657–64. <a href="https://doi.org/10.1038/s41562-017-0186-2">https://doi.org/10.1038/s41562-017-0186-2</a>.
</div>
<div id="ref-Vartiainen2011" class="csl-entry" role="doc-biblioentry">
Vartiainen, Johanna, Mia Liljeström, Miika Koskinen, Hanna Renvall, and Riitta Salmelin. 2011. <span>“Functional Magnetic Resonance Imaging Blood Oxygenation Level-Dependent Signal and Magnetoencephalography Evoked Responses Yield Different Neural Functionality in Reading.”</span> <em>The Journal of Neuroscience</em> 31 (3): 1048–58. <a href="https://doi.org/10.1523/jneurosci.3113-10.2011">https://doi.org/10.1523/jneurosci.3113-10.2011</a>.
</div>
<div id="ref-Vinckier2007" class="csl-entry" role="doc-biblioentry">
Vinckier, Fabien, Stanislas Dehaene, Antoinette Jobert, Jean Philippe Dubus, Mariano Sigman, and Laurent Cohen. 2007. <span>“Hierarchical <span>Coding</span> of <span>Letter Strings</span> in the <span>Ventral Stream</span>: <span>Dissecting</span> the <span>Inner Organization</span> of the <span>Visual Word</span>-<span>Form System</span>.”</span> <em>Neuron</em> 55 (1): 143–56. <a href="https://doi.org/10.1016/j.neuron.2007.05.031">https://doi.org/10.1016/j.neuron.2007.05.031</a>.
</div>
<div id="ref-Yamins2016" class="csl-entry" role="doc-biblioentry">
Yamins, Daniel L. K., and James J. DiCarlo. 2016. <span>“Using Goal-Driven Deep Learning Models to Understand Sensory Cortex.”</span> <em>Nature Neuroscience</em> 19 (3): 356–65. <a href="https://doi.org/10.1038/nn.4244">https://doi.org/10.1038/nn.4244</a>.
</div>
</div>
</body>
</html>
